"""Standalone script that will load data from imdb files into the app db."""

from django.core.management.base import BaseCommand, CommandError
from django.core.exceptions import ObjectDoesNotExist
from django.db import connection
from sqlalchemy import create_engine
from io import StringIO
from contextlib import closing
from scores.models import (
    Title,
    Name,
    Principal,
)

import csv
import environ
import pandas as pd

root = environ.Path(__file__) - 3
env = environ.Env()

# set up file paths
TITLE_FILE = 'title.basics.tsv'
NAME_FILE = 'name.basics.tsv'
PRINCIPAL_FILE = 'title.principals.tsv'

# test data
# DATA_PATH = str(root.path('data/test/')) + '/'

# imdb data
DATA_PATH = str(root.path('data/')) + '/'

TITLE_PATH = DATA_PATH + TITLE_FILE
NAME_PATH = DATA_PATH + NAME_FILE
PRINCIPAL_PATH = DATA_PATH + PRINCIPAL_FILE

# create db engine to run sql query generated by functions
ENGINE = create_engine(env('DATABASE_URL'), echo=True)

BAD_NAMES = set(['nm5170823', 'nm5662269', 'nm3064095', 'nm8641496', 'nm5401436', 'nm8048960', 'nm1287305', 'nm3293176', 'nm1821809', 'nm4993248', 'nm5735527', 'nm1026618', 'nm8371693', 'nm8348219', 'nm5405937', 'nm9382723', 'nm5467914', 'nm3799416', 'nm4207809', 'nm7203167', 'nm0662054', 'nm7660195', 'nm7417789', 'nm8702101', 'nm7182502', 'nm2118078', 'nm7480410', 'nm7706311', 'nm6370952', 'nm5865373', 'nm7901153', 'nm9381612', 'nm4661242', 'nm5684544', 'nm5639893', 'nm9014765', 'nm9179918', 'nm8727319', 'nm4788174', 'nm0206222', 'nm4390271', 'nm4432236', 'nm7922488', 'nm9352055', 'nm8781425', 'nm4128522', 'nm3822228', 'nm4907559', 'nm9098626', 'nm7700311', 'nm1813208', 'nm3124182', 'nm9412186', 'nm0874955', 'nm7952042', 'nm1690100', 'nm8524284', 'nm3879757', 'nm8871073', 'nm6396984', 'nm9056027', 'nm6250452', 'nm9500996', 'nm4652502', 'nm3244099', 'nm7682773', 'nm9188965', 'nm9450335', 'nm6960525', 'nm3789603', 'nm6706647', 'nm5978936', 'nm6346522', 'nm8798617', 'nm9377787', 'nm7783177', 'nm5341770', 'nm9152617', 'nm8258202', 'nm9387241', 'nm0402378', 'nm5529380', 'nm8377117', 'nm7801019', 'nm9401587', 'nm6505281', 'nm1290182', 'nm8947198', 'nm7817297', 'nm2884782', 'nm8906263', 'nm8702395', 'nm7227873', 'nm7883702', 'nm9525164', 'nm7704689', 'nm7955272', 'nm8960500', 'nm2028372', 'nm7183931', 'nm8052247', 'nm7783179', 'nm9436617', 'nm4395417', 'nm6672271', 'nm8913209', 'nm9381501', 'nm7292697', 'nm5577436', 'nm8211815', 'nm8793555', 'nm8110436', 'nm5859373', 'nm6344661', 'nm9410042', 'nm0030281', 'nm1802584', 'nm6160696', 'nm4419794', 'nm2179974', 'nm9410592', 'nm7906966', 'nm8805030', 'nm4972572', 'nm0248490', 'nm8316100', 'nm3935597', 'nm8377982', 'nm7283087', 'nm1298621', 'nm7970743', 'nm8078627', 'nm2021406', 'nm8579868', 'nm1278029', 'nm9384238', 'nm2165109', 'nm5137404', 'nm9149649', 'nm0659653', 'nm8442256', 'nm3094036', 'nm1664628', 'nm6489376', 'nm5549331', 'nm4886795', 'nm0219808', 'nm3597396', 'nm7171926', 'nm6609829', 'nm7826219', 'nm7823945', 'nm9315442', 'nm5058859', 'nm7061687', 'nm2368341', 'nm1043391', 'nm0484138', 'nm4775019', 'nm2147416', 'nm8877235', 'nm4785289', 'nm4384156', 'nm9391618', 'nm4344506', 'nm9364917', 'nm5183032', 'nm1087761', 'nm9408390', 'nm7575836', 'nm6019994', 'nm6843076', 'nm8577486', 'nm3826861', 'nm9499765', 'nm7467576', 'nm1996613', 'nm4598874', 'nm9253020', 'nm8319393', 'nm7963682', 'nm6383177', 'nm5658229', 'nm9128911', 'nm6175620', 'nm6776583', 'nm2115903', 'nm5198268', 'nm6424678', 'nm2089512', 'nm6470627', 'nm4081595', 'nm9319494', 'nm4008630', 'nm8246829', 'nm9480124', 'nm8816951', 'nm4589105', 'nm9340985', 'nm8052248', 'nm3369181', 'nm7381445', 'nm9098742', 'nm6759154', 'nm4624521', 'nm6982700', 'nm9240797', 'nm5091152', 'nm5697936', 'nm9064694', 'nm1447609', 'nm3934694', 'nm7797278', 'nm8304716', 'nm2173975', 'nm8599936', 'nm1779352', 'nm3127152', 'nm8752117', 'nm2534125', 'nm8697404', 'nm5585551', 'nm4770424', 'nm0316210', 'nm4597984', 'nm9230061', 'nm1540828', 'nm8159955', 'nm7963210', 'nm9430232', 'nm8518997', 'nm1028808', 'nm6003839', 'nm6954350', 'nm5268481', 'nm2005088', 'nm5434082', 'nm7032456', 'nm3778289', 'nm5277626', 'nm1317385', 'nm9350433', 'nm8952185', 'nm8857992', 'nm8357611', 'nm8893355', 'nm6867621', 'nm9363743', 'nm8004064', 'nm6489052', 'nm7722678', 'nm9137507', 'nm8383133', 'nm7658762', 'nm9332128', 'nm3597914', 'nm0383037', 'nm7388846', 'nm1651414', 'nm6398503', 'nm9036075', 'nm1001709', 'nm4780290', 'nm9427277', 'nm7315509', 'nm8497449', 'nm9421040', 'nm3798114', 'nm9345798', 'nm3516645', 'nm9142306', 'nm4811937', 'nm8614056', 'nm6224735', 'nm7432220', 'nm8439797', 'nm7305766', 'nm3467059', 'nm8207905', 'nm4024027', 'nm4497120', 'nm7449393', 'nm8078803', 'nm7050767', 'nm7301077', 'nm8901185', 'nm8328381', 'nm9433087', 'nm5329126', 'nm4363997', 'nm1056872', 'nm7784161', 'nm9439850', 'nm9330642', 'nm2221893', 'nm3498919', 'nm4864462', 'nm4708952', 'nm7940686', 'nm8440581', 'nm5338343', 'nm7107451', 'nm8377082', 'nm3070740', 'nm8432690', 'nm9120775', 'nm5083490', 'nm8669247', 'nm6395567', 'nm8795665', 'nm9430357', 'nm2382509', 'nm1281620', 'nm5650280', 'nm1653529', 'nm2299443', 'nm9482930', 'nm8489274', 'nm9169449', 'nm3510277', 'nm9084952', 'nm4852231', 'nm0123871', 'nm7848956', 'nm7671885', 'nm6634723', 'nm1757575', 'nm4932754', 'nm2530639', 'nm7984868', 'nm3883364', 'nm0659559', 'nm3879875', 'nm8094915', 'nm8304593', 'nm4329232', 'nm5668762', 'nm4270201', 'nm8368395', 'nm9347527', 'nm0078881', 'nm4345758', 'nm1686562', 'nm8431195', 'nm6730740', 'nm6085505', 'nm7258954', 'nm2357338', 'nm4415154', 'nm3606428', 'nm4205519', 'nm8030349', 'nm5814024', 'nm1886313', 'nm8847511', 'nm9436737', 'nm2114706', 'nm7495838', 'nm8541294', 'nm9126996', 'nm9488409', 'nm8095438', 'nm8801559', 'nm9354135', 'nm9519020', 'nm2396567', 'nm0903635', 'nm5471292', 'nm9257596', 'nm5900676', 'nm8562374', 'nm7324082', 'nm8530869', 'nm1579870', 'nm8385329', 'nm6547183', 'nm5689020', 'nm4743295', 'nm8248947', 'nm4644735', 'nm5046982', 'nm8835127', 'nm8585524', 'nm7465538', 'nm6311386', 'nm6428813', 'nm7683639', 'nm6901444', 'nm4619658', 'nm3309533', 'nm4026741', 'nm8825900', 'nm4019760', 'nm9064757', 'nm4602152', 'nm0753833', 'nm2106974', 'nm6398337', 'nm8453724', 'nm4661025', 'nm8319896', 'nm4196188', 'nm9155866', 'nm7278526', 'nm8329761', 'nm7059718', 'nm1685176', 'nm7619673', 'nm9133451'])

BAD_TITLES = set(['tt7813092', 'tt7737202', 'tt3124466', 'tt7670266', 'tt7820930', 'tt7409830', 'tt7809520', 'tt6544114', 'tt7294364', 'tt5001334', 'tt7086876', 'tt6495674', 'tt7623550', 'tt7813436', 'tt6458982', 'tt4040734', 'tt7409684', 'tt7172312', 'tt6847632', 'tt7588082', 'tt7802858', 'tt5051770', 'tt6806986', 'tt7284398', 'tt7779274', 'tt7817706', 'tt1156776', 'tt7810458', 'tt0266411', 'tt2564002', 'tt2386363', 'tt6341046', 'tt7387534', 'tt1915882'])


def tidy_split(df, column, sep='|', keep=False):
    """
    Split the values of a column and expand so the new DataFrame has one split
    value per row. Filters rows where the column is missing.

    Params
    ------
    df : pandas.DataFrame
        dataframe with the column to split and expand
    column : str
        the column to split and expand
    sep : str
        the string used to split the column's values
    keep : bool
        whether to retain the presplit value as it's own row

    Returns
    -------
    pandas.DataFrame
        Returns a dataframe with the same columns as `df`.

    Courtesy of StackOverflow
    (https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows)
    """
    indexes = list()
    new_values = list()
    df = df.dropna(subset=[column])
    for i, presplit in enumerate(df[column].astype(str)):
        values = presplit.split(sep)
        if keep and len(values) > 1:
            indexes.append(i)
            new_values.append(presplit)
        for value in values:
            indexes.append(i)
            new_values.append(value)
    new_df = df.iloc[indexes, :].copy()
    new_df[column] = new_values
    return new_df


def load_titles():
    """Read title.basics.tsv into db."""
    print('loading titles')
    Title.objects.all().delete()

    # create dataframe
    df = pd.read_csv(
        TITLE_PATH,
        sep='\t',
        header=0,
        names=[
            'id',
            'title_type',
            'primary_title',
            'original_title',
            'is_adult',
            'start_year',
            'end_year',
            'runtime_minutes',
            'genres'
        ],
        quoting=3,
        na_values=['\\N']
    )

    df['is_adult'] = df['is_adult'].map({0: False, 1: True})

    df.to_sql(Title._meta.db_table, ENGINE, if_exists='append', index=False)

    return None


def load_names():
    """Read name.basics.tsv into db."""
    print('loading names')

    # drop all records from table
    Name.objects.all().delete()

    # conn = psycopg2.connect(env('DATABASE_URL'))
    # cur = conn.cursor()
    #
    # with open(NAME_PATH, 'r') as f:
    #     next(f)  # skip the header row
    #     cur.copy_from(f, Name._meta.db_table, sep='\t')
    #
    # conn.commit()

    # sio = StringIO()
    #
    # sio.write(df.to_csv(header=False))
    #
    # sio.seek(0)
    #
    # with conn.cursor() as c:
    #     print(sio)
    #     c.copy_from(sio, Principal._meta.db_table, sep=',')
    #     conn.commit()
    df = pd.read_csv(
        NAME_PATH,
        sep='\t',
        header=0,
        names=[
            'id',
            'primary_name',
            'birth_year',
            'death_year',
            'professions',
            'known_for'
        ],
        quoting=3,
        na_values=['\\N'],
    )

    print(df)

    df.to_sql(Name._meta.db_table, ENGINE, if_exists='append', index=False)


def load_principals():
    """
    Read title.principals.tsv into db.

    principalCast is given as an array of comma-separated strings.
    This splits the array so that one row turns into multiple rows
    for insertion into the join table.
    """
    print('loading principals')
    # delete all records from table
    Principal.objects.all().delete()

    # conn = psycopg2.connect(env('DATABASE_URL'))
    # cur = conn.cursor()
    # with open(PRINCIPAL_PATH, 'r') as f:
    #     next(f)  # skip the header row
    #     cur.copy_from(f, Principal._meta.db_table, sep='\t')
    #
    # conn.commit()

    # create dataframe
    df = pd.read_csv(
        PRINCIPAL_PATH,
        sep='\t',
        header=0,
        names=['title_id', 'name_id'],
        quoting=3,
        na_values=['\\N']
    )

    # split names array in df into multiple rows
    df = tidy_split(df, 'name_id', sep=',')

    # reset index to act as pk
    df.reset_index(drop=True, inplace=True)

    # bad_data = set()
    # for row in df.itertuples():
    #     name = row[2]
    #     title = row[1]
    #     print(row[0])
    #
    #     try:
    #         Title.objects.get(id=title)
    #         pass
    #     except ObjectDoesNotExist:
    #         bad_data.add(title)
    #
    # print(bad_data)
    #
    #     try:
    #         Name.objects.get(id=name)
    #         pass
    #     except ObjectDoesNotExist:
    #         bad_data.append(name)

    # print(bad_data)
    # for data in bad_data:

    # specify order of columns
    df = df[['name_id', 'title_id']]

    # rename index to id to match table
    df.index.names = ['id']

    # model_list = []

    sio = StringIO()
    writer = csv.writer(sio, delimiter='\t')

    for row in df.itertuples():
        print(row)
        name_id = row[1]
        title_id = row[2]

        if((name_id not in BAD_NAMES) and (title_id not in BAD_TITLES)):
            writer.writerow([row[0], name_id, title_id])

    sio.seek(0)

    print('written')
    with closing(connection.cursor()) as cursor:
        cursor.copy_from(
            file=sio,
            table='scores_principal',
            sep='\t',
            columns=('id, name_id', 'title_id')
        )

    # sio = StringIO()
    #
    # sio.write(df.to_csv(header=False))
    #
    # sio.seek(0)
    #
    # with conn.cursor() as c:
    #     c.copy_from(sio, Principal._meta.db_table, sep=',')
    #     conn.commit()

    # df.to_sql(Principal._meta.db_table, ENGINE, if_exists='append')


class Command(BaseCommand):
    """Add command line function to load data when calling manage.py."""

    help = 'Reads IMDB data files into the database'

    def add_arguments(self, parser):
        """Add arg to specify file type (or all) to load into db."""
        parser.add_argument(
            'file',
            help='file to add: titles, names, principals, or all'
        )

    def handle(self, *args, **options):
        """Load all imdb data."""
        data = options['file']

        if data == 'titles' or data == 't':
            load_titles()
        elif data == 'names' or data == 'n':
            load_names()
        elif data == 'principals' or data == 'p':
            load_principals()
        elif data == 'all':
            load_titles()
            load_names()
            load_principals()
